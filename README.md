# Rinha de Backend 2025

API da Rinha de Backend 2025 desenvolvida em **Java com Spring Boot**, empacotada como imagem nativa com **GraalVM 24**.

## ğŸš€ Tecnologias

- **Java 21**
- **Spring Boot**
- **GraalVM 24 Native Image**
- **Docker & Docker Compose**
- **HAProxy** (load balancer)


## ğŸ¯ DescriÃ§Ã£o

Este projeto Ã© uma API REST, preparada para alta performance e inicializaÃ§Ã£o rÃ¡pida usando GraalVM Native Image. Em ambiente de container, utiliza Docker Compose para orquestraÃ§Ã£o e HAProxy como balanceador de carga.


## ğŸ“¦ PrÃ©-requisitos

- Docker
- Docker Compose


## ğŸ³ Executando com Docker Compose

1. Clone este repositÃ³rio:
   ```bash
   git clone https://github.com/seu-usuario/rinha-backend.git
   cd rinha-backend
   ```
2. Iniciar com Docker compose:  
   ```bash
   docker-compose -f docker-compose.yml -f docker-compose-rinha.yml up -d
   ```
  
   - `docker-compose.yml`: serviÃ§os bÃ¡sicos da aplicaÃ§Ã£o e dependÃªncias.
   - `docker-compose-rinha.yml`: configuraÃ§Ãµes especÃ­ficas da rinha (HAProxy, redes, volumes).

## âš™ï¸ Testes com k6

ApÃ³s clonar o repositÃ³rio de testes, siga estes passos:
   ```bash
   cd rinha-test
   k6 run rinha.js 
   ```
Isso gerarÃ¡ o arquivo partial-results.json com os resultados. Ã‰ necessÃ¡rio ter o k6 instalado na sua mÃ¡quina.

## Resultados

   Com base na regulamentaÃ§Ã£o da Rinha de Backend 2025, o uso total de CPU nÃ£o poderia ultrapassar 1.5 cores, e o consumo de memÃ³ria RAM deveria permanecer abaixo de 350â€¯MB.

   Utilizando GraalVM, consegui reduzir drasticamente o uso de memÃ³ria ao compilar a aplicaÃ§Ã£o diretamente para binÃ¡rio nativo, eliminando classes e pacotes nÃ£o utilizados no empacotamento final. Com isso, 
   cada instÃ¢ncia da API consome apenas 110â€¯MB de RAM e 0.65 de CPU. O HAProxy complementa com 35â€¯MB de RAM e 0.20 de CPU, totalizando 255â€¯MB de memÃ³ria RAM, ou seja, 95â€¯MB abaixo do limite, representando 
   uma reduÃ§Ã£o de cerca de 27% em relaÃ§Ã£o aos testes iniciais.

   Nos primeiros testes, a aplicaÃ§Ã£o apresentava 200 inconsistÃªncias. ApÃ³s ajustes finos, especialmente na concorrÃªncia com virtual threads, atingi zero inconsistÃªncias na soluÃ§Ã£o final â€” uma reduÃ§Ã£o de 100%.

   Em termos de desempenho, o P99 no ambiente local era de 18â€¯ms no inÃ­cio. Com ajustes no sistema e melhorias no backend, atingi um P99 de 4â€¯ms, ou seja, 99% das requisiÃ§Ãµes respondem em atÃ© 4â€¯ms. 
   Isso representa uma melhora de aproximadamente 77% no tempo de resposta.

   Utilizando WSL com Docker, os primeiros testes apresentaram um P99 entre 70â€¯ms e 90â€¯ms, impactado principalmente pela restriÃ§Ã£o de CPU, que limitava bastante o desempenho. ApÃ³s ajustes no sistema e otimizaÃ§Ãµes 
   no backend, consegui reduzir o P99 para uma faixa entre 16â€¯ms e 20â€¯ms, representando uma melhora de aproximadamente 77,5%.

   dVale destacar que o WSL impÃµe limitaÃ§Ãµes de desempenho significativas. Infelizmente, nÃ£o tive a oportunidade de testar diretamente em um ambiente Linux nativo, onde os resultados provavelmente seriam ainda melhores.

### 1. Resultado dos testes executados localmente utilizando apenas uma instÃ¢ncia da aplicaÃ§Ã£o.

   ![partial-results-local-one-instance](./img/partial-results-local-one-instance.png)
 
### 2. Resultado dos testes executados localmente utilizando nginx como load balancer e duas instÃ¢ncia da aplicaÃ§Ã£o.

   ![partial-results-local-nginx](./img/partial-results-local-nginx.png)   

### 3. Resultado dos testes executados utilizando Docker, haproxy e duas instÃ¢ncias da aplicaÃ§Ã£o empacotadas com Maven em GraalVM 24 (foi utilizado WSL, o que reduz drasticamente o desempenho).

   ![partial-results-local-nginx](./img/partial-results-local-nginx.png)  

## âš™ï¸ HAProxy

O HAProxy faz balanceamento de carga entre instÃ¢ncias da API:

- ConfiguraÃ§Ã£o: `haproxy.cfg`
- ServiÃ§o: `haproxy` no Docker Compose

